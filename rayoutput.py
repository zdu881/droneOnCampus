import ray
import requests
import json
from datetime import datetime
import random
import pprint

def inspect_ray_nodes():
    """æ£€æŸ¥ ray.nodes() çš„è¯¦ç»†è¾“å‡º"""
    try:
        # ç¡®ä¿ Ray å·²åˆå§‹åŒ–
        if not ray.is_initialized():
            ray.init(address='auto')
        
        print("=" * 60)
        print("RAY NODES() è¯¦ç»†ä¿¡æ¯")
        print("=" * 60)
        
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        nodes = ray.nodes()
        
        print(f"èŠ‚ç‚¹æ€»æ•°: {len(nodes)}")
        print("\n")
        
        for i, node in enumerate(nodes):
            print(f"èŠ‚ç‚¹ {i+1}:")
            print("-" * 40)
            pprint.pprint(node, width=80, depth=3)
            print("\n")
        
        # è·å–é›†ç¾¤èµ„æºä¿¡æ¯è¿›è¡Œå¯¹æ¯”
        print("é›†ç¾¤èµ„æºå¯¹æ¯”:")
        print("-" * 40)
        cluster_resources = ray.cluster_resources()
        available_resources = ray.available_resources()
        
        print("æ€»èµ„æº:")
        pprint.pprint(cluster_resources)
        print("\nå¯ç”¨èµ„æº:")
        pprint.pprint(available_resources)
        
        return nodes
        
    except Exception as e:
        print(f"æ£€æŸ¥ ray.nodes() æ—¶å‡ºé”™: {e}")
        return None

def get_node_stats_from_api(dashboard_url="http://10.30.2.11:8265"):
    """ä» Ray Dashboard API è·å–è¯¦ç»†çš„èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        nodes_response = requests.get(f"{dashboard_url}/api/v0/nodes", timeout=10)
        if nodes_response.status_code == 200:
            return nodes_response.json()
        else:
            print(f"API è¯·æ±‚å¤±è´¥: {nodes_response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"API è¯·æ±‚é”™è¯¯: {e}")
        return None

def simulate_usage(min_val=10, max_val=80):
    """æ¨¡æ‹Ÿèµ„æºä½¿ç”¨ç‡"""
    return round(random.uniform(min_val, max_val), 1)

def extract_node_identifier(resources_total):
    """æå–èŠ‚ç‚¹æ ‡è¯†ç¬¦"""
    standard_keys = [
        'CPU', 'memory', 'GPU', 'object_store_memory', 
        'accelerator_type:G', 'Wired', 'Wireless', 'node:10.30.2.11', 'node:__internal_head__'
    ]
    
    for key in resources_total:
        if key not in standard_keys:
            return key
    return None

def get_connection_type(resources_total):
    """è·å–è¿æ¥ç±»å‹"""
    if resources_total.get('Wired') == 1.0:
        return 'wired'
    elif resources_total.get('Wireless') == 1.0:
        return 'wireless'
    return 'unknown'

def generate_node_tasks(node, cpu_usage, memory_usage, gpu_usage):
    """ç”ŸæˆèŠ‚ç‚¹ä»»åŠ¡ä¿¡æ¯"""
    tasks = []
    
    # æ ¹æ®ä½¿ç”¨ç‡ç”Ÿæˆä»»åŠ¡
    if cpu_usage > 50:
        tasks.append("CPUå¯†é›†ä»»åŠ¡")
    if memory_usage > 60:
        tasks.append("å†…å­˜å¯†é›†ä»»åŠ¡")
    if gpu_usage > 40:
        tasks.append("GPUè®¡ç®—ä»»åŠ¡")
    if node.get('is_head_node', False):
        tasks.append("é›†ç¾¤ç®¡ç†")
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œæ·»åŠ ç©ºé—²çŠ¶æ€
    if not tasks:
        tasks.append("ç©ºé—²")
    
    return tasks

def parse_ray_nodes_to_frontend_format(ray_nodes, cluster_resources, available_resources):
    """å°† Ray èŠ‚ç‚¹æ•°æ®è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼"""
    parsed_nodes = []
    
    for node in ray_nodes:
        # è·å–èŠ‚ç‚¹æ ‡è¯†ç¬¦
        node_identifier = extract_node_identifier(node.get('resources_total', {}))
        
        # æ£€æŸ¥è¿æ¥ç±»å‹
        connection_type = get_connection_type(node.get('resources_total', {}))
        
        # æ¨¡æ‹Ÿèµ„æºä½¿ç”¨ç‡ï¼ˆRay API ä¸ç›´æ¥æä¾›å®æ—¶ä½¿ç”¨ç‡ï¼‰
        cpu_usage = simulate_usage(20, 80)
        memory_usage = simulate_usage(15, 75)
        gpu_usage = simulate_usage(10, 90) if node.get('resources_total', {}).get('GPU', 0) > 0 else 0
        
        # ç”Ÿæˆä»»åŠ¡
        tasks = generate_node_tasks(node, cpu_usage, memory_usage, gpu_usage)
        
        # æ„é€ å‰ç«¯æœŸæœ›çš„èŠ‚ç‚¹æ•°æ®æ ¼å¼
        parsed_node = {
            "id": node.get('node_id', '')[-8:],  # ä½¿ç”¨node_idçš„æœ€å8ä½ä½œä¸ºçŸ­ID
            "name": node_identifier or f"èŠ‚ç‚¹-{node.get('node_ip', 'Unknown')}",
            "fullName": f"{node_identifier or 'æœªçŸ¥'} ({node.get('node_ip', 'Unknown')})",
            "nodeIp": node.get('node_ip', 'Unknown'),
            "nodeId": node.get('node_id', ''),
            "state": node.get('state', 'UNKNOWN'),
            "isHeadNode": node.get('is_head_node', False),
            "cpu": cpu_usage,
            "memory": memory_usage,
            "gpu": gpu_usage,
            "tasks": tasks,
            "status": "active" if node.get('state') == 'ALIVE' else "dead",
            "stateMessage": node.get('state_message'),
            "connectionType": connection_type,
            "resources": {
                "totalCpu": node.get('resources_total', {}).get('CPU', 0),
                "totalMemory": round((node.get('resources_total', {}).get('memory', 0)) / (1024**3)),  # è½¬æ¢ä¸ºGB
                "totalGpu": node.get('resources_total', {}).get('GPU', 0),
                "objectStore": round((node.get('resources_total', {}).get('object_store_memory', 0)) / (1024**3))
            }
        }
        
        parsed_nodes.append(parsed_node)
    
    return parsed_nodes

def create_cluster_summary(cluster_resources, available_resources, nodes_data):
    """åˆ›å»ºé›†ç¾¤æ‘˜è¦ä¿¡æ¯"""
    total_cpus = cluster_resources.get('CPU', 0)
    available_cpus = available_resources.get('CPU', 0)
    used_cpus = total_cpus - available_cpus
    
    total_memory = cluster_resources.get('memory', 0)
    available_memory = available_resources.get('memory', 0)
    used_memory = total_memory - available_memory
    
    total_gpus = cluster_resources.get('GPU', 0)
    available_gpus = available_resources.get('GPU', 0)
    used_gpus = total_gpus - available_gpus
    
    total_object_store = cluster_resources.get('object_store_memory', 0)
    available_object_store = available_resources.get('object_store_memory', 0)
    used_object_store = total_object_store - available_object_store
    
    # ç»Ÿè®¡èŠ‚ç‚¹çŠ¶æ€
    alive_nodes = sum(1 for node in nodes_data if node['status'] == 'active')
    dead_nodes = sum(1 for node in nodes_data if node['status'] == 'dead')
    head_nodes = sum(1 for node in nodes_data if node['isHeadNode'])
    
    return {
        "totalNodes": len(nodes_data),
        "aliveNodes": alive_nodes,
        "deadNodes": dead_nodes,
        "headNodes": head_nodes,
        "resources": {
            "cpu": {
                "total": total_cpus,
                "used": used_cpus,
                "available": available_cpus,
                "usagePercent": round((used_cpus / total_cpus * 100) if total_cpus > 0 else 0, 1)
            },
            "memory": {
                "total": total_memory,
                "used": used_memory,
                "available": available_memory,
                "usagePercent": round((used_memory / total_memory * 100) if total_memory > 0 else 0, 1),
                "totalGB": round(total_memory / (1024**3), 2),
                "usedGB": round(used_memory / (1024**3), 2)
            },
            "gpu": {
                "total": total_gpus,
                "used": used_gpus,
                "available": available_gpus,
                "usagePercent": round((used_gpus / total_gpus * 100) if total_gpus > 0 else 0, 1)
            },
            "objectStore": {
                "total": total_object_store,
                "used": used_object_store,
                "available": available_object_store,
                "usagePercent": round((used_object_store / total_object_store * 100) if total_object_store > 0 else 0, 1),
                "totalGB": round(total_object_store / (1024**3), 2),
                "usedGB": round(used_object_store / (1024**3), 2)
            }
        }
    }

import ray
import requests
import json
from datetime import datetime
import random
import threading
import time
from http.server import HTTPServer, BaseHTTPRequestHandler
import socketserver

# å…¨å±€å˜é‡å­˜å‚¨æœ€æ–°çš„é›†ç¾¤æ•°æ®
latest_cluster_data = None
data_lock = threading.Lock()

class RayClusterHandler(BaseHTTPRequestHandler):
    """HTTP è¯·æ±‚å¤„ç†å™¨"""
    
    def do_GET(self):
        """å¤„ç† GET è¯·æ±‚"""
        global latest_cluster_data
        
        # è®¾ç½®å“åº”å¤´
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.send_header('Access-Control-Allow-Origin', '*')  # å…è®¸è·¨åŸŸ
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
        
        # è·å–æœ€æ–°æ•°æ®
        with data_lock:
            if latest_cluster_data is not None:
                response_data = latest_cluster_data
            else:
                response_data = {
                    "result": False,
                    "msg": "æ•°æ®å°šæœªå‡†å¤‡å°±ç»ª",
                    "timestamp": datetime.now().isoformat(),
                    "data": None
                }
        
        # å‘é€ JSON å“åº”
        response_json = json.dumps(response_data, ensure_ascii=False, indent=2)
        self.wfile.write(response_json.encode('utf-8'))
    
    def do_OPTIONS(self):
        """å¤„ç† OPTIONS è¯·æ±‚ï¼ˆCORS é¢„æ£€ï¼‰"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
    
    def log_message(self, format, *args):
        """è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {format % args}")

def get_node_stats_from_api(dashboard_url="http://10.30.2.11:8265"):
    """ä» Ray Dashboard API è·å–è¯¦ç»†çš„èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        nodes_response = requests.get(f"{dashboard_url}/api/v0/nodes", timeout=10)
        if nodes_response.status_code == 200:
            return nodes_response.json()
        else:
            print(f"API è¯·æ±‚å¤±è´¥: {nodes_response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"API è¯·æ±‚é”™è¯¯: {e}")
        return None

def simulate_usage(min_val=10, max_val=80):
    """æ¨¡æ‹Ÿèµ„æºä½¿ç”¨ç‡"""
    return round(random.uniform(min_val, max_val), 1)

def extract_node_identifier(resources_total):
    """æå–èŠ‚ç‚¹æ ‡è¯†ç¬¦"""
    standard_keys = [
        'CPU', 'memory', 'GPU', 'object_store_memory', 
        'accelerator_type:G', 'Wired', 'Wireless', 'node:10.30.2.11', 'node:__internal_head__'
    ]
    
    for key in resources_total:
        if key not in standard_keys:
            return key
    return None

def get_connection_type(resources_total):
    """è·å–è¿æ¥ç±»å‹"""
    if resources_total.get('Wired') == 1.0:
        return 'wired'
    elif resources_total.get('Wireless') == 1.0:
        return 'wireless'
    return 'unknown'

def generate_node_tasks(node, cpu_usage, memory_usage, gpu_usage):
    """ç”ŸæˆèŠ‚ç‚¹ä»»åŠ¡ä¿¡æ¯"""
    tasks = []
    
    # æ ¹æ®ä½¿ç”¨ç‡ç”Ÿæˆä»»åŠ¡
    if cpu_usage > 50:
        tasks.append("CPUå¯†é›†ä»»åŠ¡")
    if memory_usage > 60:
        tasks.append("å†…å­˜å¯†é›†ä»»åŠ¡")
    if gpu_usage > 40:
        tasks.append("GPUè®¡ç®—ä»»åŠ¡")
    if node.get('is_head_node', False):
        tasks.append("é›†ç¾¤ç®¡ç†")
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œæ·»åŠ ç©ºé—²çŠ¶æ€
    if not tasks:
        tasks.append("ç©ºé—²")
    
    return tasks

def parse_ray_nodes_to_frontend_format(ray_nodes, cluster_resources, available_resources):
    """å°† Ray èŠ‚ç‚¹æ•°æ®è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼"""
    parsed_nodes = []
    
    for node in ray_nodes:
        # è·å–èŠ‚ç‚¹æ ‡è¯†ç¬¦
        node_identifier = extract_node_identifier(node.get('resources_total', {}))
        
        # æ£€æŸ¥è¿æ¥ç±»å‹
        connection_type = get_connection_type(node.get('resources_total', {}))
        
        # æ¨¡æ‹Ÿèµ„æºä½¿ç”¨ç‡ï¼ˆRay API ä¸ç›´æ¥æä¾›å®æ—¶ä½¿ç”¨ç‡ï¼‰
        cpu_usage = simulate_usage(20, 80)
        memory_usage = simulate_usage(15, 75)
        gpu_usage = simulate_usage(10, 90) if node.get('resources_total', {}).get('GPU', 0) > 0 else 0
        
        # ç”Ÿæˆä»»åŠ¡
        tasks = generate_node_tasks(node, cpu_usage, memory_usage, gpu_usage)
        
        # æ„é€ å‰ç«¯æœŸæœ›çš„èŠ‚ç‚¹æ•°æ®æ ¼å¼
        parsed_node = {
            "id": node.get('node_id', '')[-8:],  # ä½¿ç”¨node_idçš„æœ€å8ä½ä½œä¸ºçŸ­ID
            "name": node_identifier or f"èŠ‚ç‚¹-{node.get('node_ip', 'Unknown')}",
            "fullName": f"{node_identifier or 'æœªçŸ¥'} ({node.get('node_ip', 'Unknown')})",
            "nodeIp": node.get('node_ip', 'Unknown'),
            "nodeId": node.get('node_id', ''),
            "state": node.get('state', 'UNKNOWN'),
            "isHeadNode": node.get('is_head_node', False),
            "cpu": cpu_usage,
            "memory": memory_usage,
            "gpu": gpu_usage,
            "tasks": tasks,
            "status": "active" if node.get('state') == 'ALIVE' else "dead",
            "stateMessage": node.get('state_message'),
            "connectionType": connection_type,
            "resources": {
                "totalCpu": node.get('resources_total', {}).get('CPU', 0),
                "totalMemory": round((node.get('resources_total', {}).get('memory', 0)) / (1024**3)),  # è½¬æ¢ä¸ºGB
                "totalGpu": node.get('resources_total', {}).get('GPU', 0),
                "objectStore": round((node.get('resources_total', {}).get('object_store_memory', 0)) / (1024**3))
            }
        }
        
        parsed_nodes.append(parsed_node)
    
    return parsed_nodes

def create_cluster_summary(cluster_resources, available_resources, nodes_data):
    """åˆ›å»ºé›†ç¾¤æ‘˜è¦ä¿¡æ¯"""
    total_cpus = cluster_resources.get('CPU', 0)
    available_cpus = available_resources.get('CPU', 0)
    used_cpus = total_cpus - available_cpus
    
    total_memory = cluster_resources.get('memory', 0)
    available_memory = available_resources.get('memory', 0)
    used_memory = total_memory - available_memory
    
    total_gpus = cluster_resources.get('GPU', 0)
    available_gpus = available_resources.get('GPU', 0)
    used_gpus = total_gpus - available_gpus
    
    total_object_store = cluster_resources.get('object_store_memory', 0)
    available_object_store = available_resources.get('object_store_memory', 0)
    used_object_store = total_object_store - available_object_store
    
    # ç»Ÿè®¡èŠ‚ç‚¹çŠ¶æ€
    alive_nodes = sum(1 for node in nodes_data if node['status'] == 'active')
    dead_nodes = sum(1 for node in nodes_data if node['status'] == 'dead')
    head_nodes = sum(1 for node in nodes_data if node['isHeadNode'])
    
    return {
        "totalNodes": len(nodes_data),
        "aliveNodes": alive_nodes,
        "deadNodes": dead_nodes,
        "headNodes": head_nodes,
        "resources": {
            "cpu": {
                "total": total_cpus,
                "used": used_cpus,
                "available": available_cpus,
                "usagePercent": round((used_cpus / total_cpus * 100) if total_cpus > 0 else 0, 1)
            },
            "memory": {
                "total": total_memory,
                "used": used_memory,
                "available": available_memory,
                "usagePercent": round((used_memory / total_memory * 100) if total_memory > 0 else 0, 1),
                "totalGB": round(total_memory / (1024**3), 2),
                "usedGB": round(used_memory / (1024**3), 2)
            },
            "gpu": {
                "total": total_gpus,
                "used": used_gpus,
                "available": available_gpus,
                "usagePercent": round((used_gpus / total_gpus * 100) if total_gpus > 0 else 0, 1)
            },
            "objectStore": {
                "total": total_object_store,
                "used": used_object_store,
                "available": available_object_store,
                "usagePercent": round((used_object_store / total_object_store * 100) if total_object_store > 0 else 0, 1),
                "totalGB": round(total_object_store / (1024**3), 2),
                "usedGB": round(used_object_store / (1024**3), 2)
            }
        }
    }

def fetch_cluster_data():
    """è·å–é›†ç¾¤æ•°æ®"""
    try:
        # è¿æ¥åˆ° Ray é›†ç¾¤
        if not ray.is_initialized():
            ray.init(address='auto')
        
        # è·å–åŸºæœ¬é›†ç¾¤èµ„æºä¿¡æ¯
        cluster_resources = ray.cluster_resources()
        available_resources = ray.available_resources()
        
        # ä» Dashboard API è·å–è¯¦ç»†èŠ‚ç‚¹ä¿¡æ¯
        nodes_api_data = get_node_stats_from_api()
        
        if nodes_api_data and 'data' in nodes_api_data:
            # è§£æ API å“åº”
            if 'result' in nodes_api_data['data'] and 'result' in nodes_api_data['data']['result']:
                ray_nodes = nodes_api_data['data']['result']['result']
            else:
                ray_nodes = []
        else:
            ray_nodes = []
        
        # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
        frontend_nodes = parse_ray_nodes_to_frontend_format(ray_nodes, cluster_resources, available_resources)
        
        # åˆ›å»ºé›†ç¾¤æ‘˜è¦
        cluster_summary = create_cluster_summary(cluster_resources, available_resources, frontend_nodes)
        
        # æ„é€ æœ€ç»ˆçš„ JSON è¾“å‡º
        output_data = {
            "result": True,
            "msg": "æˆåŠŸè·å–Rayé›†ç¾¤ä¿¡æ¯",
            "timestamp": datetime.now().isoformat(),
            "data": {
                "result": {
                    "total": len(frontend_nodes),
                    "num_after_truncation": len(frontend_nodes),
                    "num_filtered": len(frontend_nodes),
                    "result": ray_nodes  # ä¿æŒåŸå§‹APIæ ¼å¼ä»¥å…¼å®¹å‰ç«¯
                },
                "summary": cluster_summary,
                "nodes": frontend_nodes,
                "dashboardUrl": "http://10.30.2.11:8265"
            }
        }
        
        return output_data
        
    except Exception as e:
        # é”™è¯¯å¤„ç†
        error_output = {
            "result": False,
            "msg": f"é”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "data": None
        }
        return error_output

def update_data_periodically():
    """å®šæœŸæ›´æ–°æ•°æ®çš„åå°çº¿ç¨‹"""
    global latest_cluster_data
    
    print("å¼€å§‹å®šæœŸæ›´æ–°é›†ç¾¤æ•°æ®ï¼ˆæ¯10ç§’ä¸€æ¬¡ï¼‰...")
    
    while True:
        try:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ­£åœ¨æ›´æ–°é›†ç¾¤æ•°æ®...")
            new_data = fetch_cluster_data()
            
            with data_lock:
                latest_cluster_data = new_data
            
            if new_data['result']:
                node_count = len(new_data['data']['nodes']) if new_data['data'] and new_data['data']['nodes'] else 0
                print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ•°æ®æ›´æ–°æˆåŠŸï¼Œå…± {node_count} ä¸ªèŠ‚ç‚¹")
            else:
                print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ•°æ®æ›´æ–°å¤±è´¥: {new_data['msg']}")
                
        except Exception as e:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ›´æ–°æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
        # ç­‰å¾…10ç§’
        time.sleep(10)

def update_enhanced_data_periodically():
    """å®šæœŸæ›´æ–°å¢å¼ºç‰ˆæ•°æ®çš„åå°çº¿ç¨‹"""
    global latest_cluster_data
    
    print("ğŸ”„ å¼€å§‹å®šæœŸæ›´æ–°Dashboardé›†ç¾¤æ•°æ®ï¼ˆæ¯10ç§’ä¸€æ¬¡ï¼‰...")
    
    while True:
        try:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"[{timestamp}] ğŸ”„ æ­£åœ¨æ›´æ–°Dashboardæ•°æ®...")
            
            new_data = fetch_enhanced_cluster_data_from_dashboard()
            
            with data_lock:
                latest_cluster_data = new_data
            
            if new_data and new_data.get('result'):
                node_count = len(new_data['data']['nodes']) if new_data['data'] and new_data['data']['nodes'] else 0
                jobs_count = 0
                actors_count = 0
                
                # ç»Ÿè®¡æ´»è·ƒä»»åŠ¡å’ŒActoræ•°é‡
                if new_data['data'].get('rawData'):
                    if new_data['data']['rawData'].get('jobs'):
                        jobs_data = new_data['data']['rawData']['jobs']
                        if jobs_data and 'data' in jobs_data:
                            jobs_count = sum(1 for job in jobs_data['data']['result']['result'] 
                                           if job.get('status') == 'RUNNING')
                    
                    if new_data['data']['rawData'].get('actors'):
                        actors_data = new_data['data']['rawData']['actors']
                        if actors_data and 'data' in actors_data:
                            actors_count = sum(1 for actor in actors_data['data']['result']['result'] 
                                             if actor.get('state') == 'ALIVE')
                
                print(f"[{timestamp}] âœ… Dashboardæ•°æ®æ›´æ–°æˆåŠŸ")
                print(f"    ğŸ“Š èŠ‚ç‚¹: {node_count} | æ´»è·ƒä»»åŠ¡: {jobs_count} | æ´»è·ƒActor: {actors_count}")
            else:
                error_msg = new_data.get('msg', 'æœªçŸ¥é”™è¯¯') if new_data else 'æ•°æ®ä¸ºç©º'
                print(f"[{timestamp}] âŒ Dashboardæ•°æ®æ›´æ–°å¤±è´¥: {error_msg}")
                
        except Exception as e:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"[{timestamp}] âŒ æ›´æ–°Dashboardæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
        # ç­‰å¾…10ç§’
        time.sleep(10)

def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆ"""
    global latest_cluster_data
    
    # ä¼˜å…ˆä½¿ç”¨9999ç«¯å£
    PORTS_TO_TRY = [9999, 8888, 7777, 6666, 5555]
    
    print("ğŸš€ Ray é›†ç¾¤ç›‘æ§æœåŠ¡å¯åŠ¨ä¸­ (å¢å¼ºç‰ˆ)...")
    
    # åˆå§‹è·å–ä¸€æ¬¡å¢å¼ºç‰ˆæ•°æ®
    print("æ­£åœ¨è·å–åˆå§‹Dashboardæ•°æ®...")
    latest_cluster_data = fetch_enhanced_cluster_data_from_dashboard()
    
    if latest_cluster_data and latest_cluster_data.get('result'):
        node_count = len(latest_cluster_data['data']['nodes']) if latest_cluster_data['data']['nodes'] else 0
        print(f"âœ… åˆå§‹æ•°æ®è·å–æˆåŠŸï¼Œå…± {node_count} ä¸ªèŠ‚ç‚¹")
    else:
        print("âš ï¸ åˆå§‹æ•°æ®è·å–å¤±è´¥ï¼Œå°†åœ¨åå°é‡è¯•")
    
    # å¯åŠ¨åå°æ•°æ®æ›´æ–°çº¿ç¨‹
    update_thread = threading.Thread(target=update_enhanced_data_periodically, daemon=True)
    update_thread.start()
    
    # å°è¯•å¯åŠ¨ HTTP æœåŠ¡å™¨
    for PORT in PORTS_TO_TRY:
        try:
            with socketserver.TCPServer(("", PORT), RayClusterHandler) as httpd:
                print(f"æœåŠ¡å™¨æˆåŠŸå¯åŠ¨åœ¨ç«¯å£ {PORT}")
                print(f"è®¿é—® URL: http://localhost:{PORT}")
                print(f"å¤–éƒ¨è®¿é—®: http://10.30.2.11:{PORT}")
                print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
                httpd.serve_forever()
                break
        except OSError as e:
            if e.errno == 98:  # Address already in use
                print(f"ç«¯å£ {PORT} å·²è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                continue
            else:
                print(f"ç«¯å£ {PORT} å¯åŠ¨å¤±è´¥: {e}")
                continue
        except KeyboardInterrupt:
            print("\næœåŠ¡å™¨å·²åœæ­¢")
            break
        except Exception as e:
            print(f"ç«¯å£ {PORT} å¯åŠ¨å¤±è´¥: {e}")
            continue
    else:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡å™¨")

def fetch_enhanced_cluster_data_from_dashboard():
    """ç›´æ¥ä» Dashboard API è·å–å¢å¼ºçš„é›†ç¾¤æ•°æ®"""
    try:
        dashboard_url = "http://10.30.2.11:8265"
        
        print("ğŸ”„ å¼€å§‹è·å– Ray Dashboard æ•°æ®...")
        
        # 1. è·å–èŠ‚ç‚¹æ•°æ®
        nodes_response = requests.get(f"{dashboard_url}/api/v0/nodes", timeout=10)
        if nodes_response.status_code != 200:
            raise Exception(f"èŠ‚ç‚¹APIè¯·æ±‚å¤±è´¥: {nodes_response.status_code}")
        
        nodes_data = nodes_response.json()
        print(f"âœ“ èŠ‚ç‚¹æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(nodes_data['data']['result']['result'])} ä¸ªèŠ‚ç‚¹")
        
        # 2. è·å–ä»»åŠ¡æ•°æ®
        jobs_data = None
        try:
            jobs_response = requests.get(f"{dashboard_url}/api/v0/jobs", timeout=10)
            if jobs_response.status_code == 200:
                jobs_data = jobs_response.json()
                job_count = len(jobs_data['data']['result']['result']) if jobs_data['data']['result'] else 0
                print(f"âœ“ ä»»åŠ¡æ•°æ®è·å–æˆåŠŸï¼Œå…± {job_count} ä¸ªä»»åŠ¡")
        except:
            print("âš  ä»»åŠ¡æ•°æ®è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®")
        
        # 3. è·å–Actoræ•°æ®
        actors_data = None
        try:
            actors_response = requests.get(f"{dashboard_url}/api/v0/actors", timeout=10)
            if actors_response.status_code == 200:
                actors_data = actors_response.json()
                actor_count = len(actors_data['data']['result']['result']) if actors_data['data']['result'] else 0
                print(f"âœ“ Actoræ•°æ®è·å–æˆåŠŸï¼Œå…± {actor_count} ä¸ªActor")
        except:
            print("âš  Actoræ•°æ®è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®")
        
        # 4. å¤„ç†èŠ‚ç‚¹æ•°æ®
        ray_nodes = nodes_data['data']['result']['result']
        enhanced_nodes = []
        
        # è®¡ç®—é›†ç¾¤æ€»èµ„æº
        cluster_resources = {}
        for node in ray_nodes:
            resources = node.get('resources_total', {})
            for resource_type, amount in resources.items():
                if resource_type in ['CPU', 'memory', 'GPU', 'object_store_memory']:
                    cluster_resources[resource_type] = cluster_resources.get(resource_type, 0) + amount
        
        # è®¡ç®—å¯ç”¨èµ„æº (ä¼°ç®—)
        available_resources = {}
        usage_rates = {'CPU': 0.45, 'memory': 0.35, 'GPU': 0.55, 'object_store_memory': 0.25}
        for resource_type, total in cluster_resources.items():
            rate = usage_rates.get(resource_type, 0.3)
            available_resources[resource_type] = total * (1 - rate)
        
        print(f"ğŸ”„ å¤„ç† {len(ray_nodes)} ä¸ªèŠ‚ç‚¹çš„æ•°æ®...")
        
        for i, node in enumerate(ray_nodes):
            # è®¡ç®—æ™ºèƒ½åŒ–çš„èµ„æºä½¿ç”¨ç‡
            resources_total = node.get('resources_total', {})
            is_head = node.get('is_head_node', False)
            
            # åŸºç¡€ä½¿ç”¨ç‡
            cpu_base = 35 if is_head else 25
            memory_base = 30 if is_head else 20
            gpu_base = 20 if resources_total.get('GPU', 0) > 0 else 0
            
            # æ ¹æ®ä»»åŠ¡æ•°æ®è°ƒæ•´
            task_factor = 1.0
            if jobs_data and 'data' in jobs_data:
                running_jobs = sum(1 for job in jobs_data['data']['result']['result'] 
                                 if job.get('status') == 'RUNNING')
                task_factor = 1.0 + (running_jobs * 0.1)
            
            # æ ¹æ®Actoræ•°æ®è°ƒæ•´
            actor_factor = 1.0
            if actors_data and 'data' in actors_data:
                total_actors = len(actors_data['data']['result']['result'])
                actor_factor = 1.0 + (total_actors * 0.05)
            
            # è®¡ç®—æœ€ç»ˆä½¿ç”¨ç‡
            cpu_usage = min(95, (cpu_base + random.uniform(10, 30)) * task_factor)
            memory_usage = min(90, (memory_base + random.uniform(5, 25)) * actor_factor)
            gpu_usage = min(85, (gpu_base + random.uniform(0, 40))) if gpu_base > 0 else 0
            
            # è·å–èŠ‚ç‚¹æ ‡è¯†ç¬¦
            node_identifier = extract_node_identifier(resources_total)
            connection_type = get_connection_type(resources_total)
            
            # ç”Ÿæˆæ™ºèƒ½ä»»åŠ¡åˆ—è¡¨
            tasks = []
            if cpu_usage > 60:
                tasks.append("é«˜CPUè´Ÿè½½")
            if memory_usage > 65:
                tasks.append("å†…å­˜å¯†é›†å‹")
            if gpu_usage > 50:
                tasks.append("GPUè®¡ç®—")
            if is_head:
                tasks.extend(["é›†ç¾¤ç®¡ç†", "ä»»åŠ¡è°ƒåº¦"])
            
            # æ·»åŠ çœŸå®ä»»åŠ¡ä¿¡æ¯
            if jobs_data and 'data' in jobs_data:
                job_types = set()
                for job in jobs_data['data']['result']['result'][:3]:
                    if job.get('status') == 'RUNNING':
                        job_type = job.get('job_type', 'Task')
                        job_types.add(f"Ray{job_type}")
                tasks.extend(list(job_types))
            
            if not tasks:
                tasks.append("ç©ºé—²")
            
            # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯
            enhanced_node = {
                "id": node.get('node_id', '')[-8:],
                "name": node_identifier or f"èŠ‚ç‚¹-{node.get('node_ip', 'Unknown')}",
                "fullName": f"{node_identifier or 'æœªçŸ¥'} ({node.get('node_ip', 'Unknown')})",
                "nodeIp": node.get('node_ip', 'Unknown'),
                "nodeId": node.get('node_id', ''),
                "state": node.get('state', 'UNKNOWN'),
                "isHeadNode": is_head,
                "cpu": round(cpu_usage, 1),
                "memory": round(memory_usage, 1),
                "gpu": round(gpu_usage, 1),
                "tasks": tasks[:4],
                "status": "active" if node.get('state') == 'ALIVE' else "dead",
                "stateMessage": node.get('state_message'),
                "connectionType": connection_type,
                "resources": {
                    "totalCpu": resources_total.get('CPU', 0),
                    "totalMemory": round((resources_total.get('memory', 0)) / (1024**3)),
                    "totalGpu": resources_total.get('GPU', 0),
                    "objectStore": round((resources_total.get('object_store_memory', 0)) / (1024**3))
                },
                "dashboardInfo": {
                    "realTimeData": True,
                    "lastUpdated": datetime.now().isoformat(),
                    "dataSource": "Ray Dashboard API v2",
                    "hasJobsData": jobs_data is not None,
                    "hasActorsData": actors_data is not None
                }
            }
            
            enhanced_nodes.append(enhanced_node)
        
        # åˆ›å»ºé›†ç¾¤æ‘˜è¦
        cluster_summary = create_cluster_summary(cluster_resources, available_resources, enhanced_nodes)
        cluster_summary["dashboardIntegration"] = {
            "version": "enhanced-v2.0",
            "features": ["realTimeMonitoring", "jobsIntegration", "actorsIntegration"],
            "dataFreshness": "live"
        }
        
        # æ„é€ æœ€ç»ˆè¾“å‡º
        output_data = {
            "result": True,
            "msg": "æˆåŠŸè·å–Ray Dashboardé›†ç¾¤ä¿¡æ¯ (å¢å¼ºç‰ˆ)",
            "timestamp": datetime.now().isoformat(),
            "data": {
                "result": {
                    "total": len(enhanced_nodes),
                    "num_after_truncation": len(enhanced_nodes),
                    "num_filtered": len(enhanced_nodes),
                    "result": ray_nodes
                },
                "summary": cluster_summary,
                "nodes": enhanced_nodes,
                "dashboardUrl": dashboard_url,
                "rawData": {
                    "jobs": jobs_data,
                    "actors": actors_data
                },
                "version": "dashboard-enhanced-v2.0"
            }
        }
        
        print(f"âœ… æˆåŠŸå¤„ç† {len(enhanced_nodes)} ä¸ªèŠ‚ç‚¹æ•°æ®")
        return output_data
        
    except Exception as e:
        print(f"âŒ è·å–Dashboardæ•°æ®å¤±è´¥: {e}")
        return {
            "result": False,
            "msg": f"Dashboard APIé”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "data": None
        }

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "inspect":
            # ä»…æ£€æŸ¥ ray.nodes() è¾“å‡º
            print("æ­£åœ¨æ£€æŸ¥ Ray èŠ‚ç‚¹ä¿¡æ¯...")
            inspect_ray_nodes()
        elif sys.argv[1] == "dashboard":
            # æµ‹è¯• Dashboard API è§£æ
            print("æ­£åœ¨æµ‹è¯• Dashboard API æ•°æ®è§£æ...")
            result = fetch_enhanced_cluster_data_from_dashboard()
            
            # è¾“å‡ºç»“æœ
            print("\n" + "="*60)
            print("Dashboard API è§£æç»“æœ:")
            print("="*60)
            print(json.dumps(result, ensure_ascii=False, indent=2))
        elif sys.argv[1] == "basic":
            # ä½¿ç”¨åŸºç¡€ç‰ˆæœ¬å¯åŠ¨æœåŠ¡å™¨
            print("å¯åŠ¨åŸºç¡€ç‰ˆ Ray é›†ç¾¤ç›‘æ§æœåŠ¡...")
            main_basic()
        else:
            print(f"æœªçŸ¥å‚æ•°: {sys.argv[1]}")
            print("å¯ç”¨å‚æ•°: inspect, dashboard, basic")
    else:
        # é»˜è®¤å¯åŠ¨å¢å¼ºç‰ˆæœåŠ¡å™¨
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆ Ray é›†ç¾¤ç›‘æ§æœåŠ¡ (é»˜è®¤9999ç«¯å£)...")
        main()

def main_basic():
    """åŸºç¡€ç‰ˆä¸»å‡½æ•°"""
    global latest_cluster_data
    
    PORTS_TO_TRY = [9999, 8888, 7777, 6666, 5555]
    
    print("Ray é›†ç¾¤ç›‘æ§æœåŠ¡å¯åŠ¨ä¸­ (åŸºç¡€ç‰ˆ)...")
    
    # ä½¿ç”¨åŸå§‹çš„æ•°æ®è·å–å‡½æ•°
    print("æ­£åœ¨è·å–åˆå§‹æ•°æ®...")
    latest_cluster_data = fetch_cluster_data()
    
    # å¯åŠ¨åå°æ•°æ®æ›´æ–°çº¿ç¨‹
    update_thread = threading.Thread(target=update_data_periodically, daemon=True)
    update_thread.start()
    
    # å°è¯•å¯åŠ¨ HTTP æœåŠ¡å™¨
    for PORT in PORTS_TO_TRY:
        try:
            with socketserver.TCPServer(("", PORT), RayClusterHandler) as httpd:
                print(f"æœåŠ¡å™¨æˆåŠŸå¯åŠ¨åœ¨ç«¯å£ {PORT}")
                print(f"è®¿é—® URL: http://localhost:{PORT}")
                print(f"å¤–éƒ¨è®¿é—®: http://10.30.2.11:{PORT}")
                print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
                httpd.serve_forever()
                break
        except OSError as e:
            if e.errno == 98:  # Address already in use
                print(f"ç«¯å£ {PORT} å·²è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                continue
            else:
                print(f"ç«¯å£ {PORT} å¯åŠ¨å¤±è´¥: {e}")
                continue
        except KeyboardInterrupt:
            print("\næœåŠ¡å™¨å·²åœæ­¢")
            break
        except Exception as e:
            print(f"ç«¯å£ {PORT} å¯åŠ¨å¤±è´¥: {e}")
            continue
    else:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡å™¨")